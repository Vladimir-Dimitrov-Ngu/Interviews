{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSSOb3GVN60F",
        "outputId": "84c4cb04-d742-433e-ceac-1c933dfdb88f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQnRUIs8VZ8j",
        "outputId": "75826e51-9617-4599-e302-91f2f9fc9b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ],
      "source": [
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgP4Eqc9RVpL",
        "outputId": "e9a30e08-cab1-4ba2-f211-6364b900fbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/NLP/test.zip\n",
            "  inflating: test_tin.csv            \n",
            "  inflating: train_tin.csv           \n",
            "  inflating: Тестовое задание по NLP от BST.docx  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/NLP/test.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jtg7iJSYOdZ-"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv(r'/content/train_tin.csv', encoding='Windows-1251', sep=',')\n",
        "test = pd.read_csv(r'/content/test_tin.csv', encoding='Windows-1251', sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy-z4JHETZjf",
        "outputId": "92f0ab89-2e85-4ad3-dcea-17dc7734281a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер датасета 3950\n",
            "Позитивных наблюдений 1975, негативных наблюдений 1975\n"
          ]
        }
      ],
      "source": [
        "print(f'Размер датасета {train.shape[0]}')\n",
        "print(f'Позитивных наблюдений {train[train.isPositive == 1].shape[0]}, негативных наблюдений {train[train.isPositive == 0].shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wJwr0BsoOv1l"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train.text, train.isPositive, random_state=13, shuffle=True)\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnDxfRRuULaB"
      },
      "source": [
        "Векторизуем наши данные с помощью CountVectorizer и TfidfVectorizer.\n",
        "CountVectorizer делает простую вещь:\n",
        "\n",
        "## CountVectorizer\n",
        "Cтроит для каждого документа (каждой пришедшей ему строки) вектор размерности n, где n -- количество слов или n-грам во всём корпусе\n",
        "заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
        "\n",
        "## TF-IDF\n",
        "Эта мера, которая характеризует важность слова для конкретного текста. Рассчитывается следующим образом: для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
        "$$\n",
        "где $C(t | d)$ - число вхождений слова $t$ в текст $d$.\n",
        "\n",
        "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
        "$$\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right)\n",
        "$$\n",
        "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
        "\n",
        "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "Интерпретировать данную формулу можно так: чем чаще данное слово встречается в данном тексте и чем реже в остальных, тем важнее оно для этого текста.\n",
        "\n",
        "Отмечу, что в качестве TF и IDF можно использовать другие [определения](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition).\n",
        "\n",
        "В целом можно было использовать любой понравившейся нам классифактор, я буду использовать логарфимическую регрессию (для ускорения процесса будут браться первые 200 итераций).\n",
        "\n",
        "Ошибка будет выводиться с помощью confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcILPR8tHRAm"
      },
      "source": [
        "ngram_range=(1, 1) — униграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_ULqOWrUcRo"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(X_train)\n",
        "bow_test = vec.transform(X_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFOKtl1vUiX9",
        "outputId": "6eaeca9b-046a-46f3-b7e2-0602c0792d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       525\n",
            "           1       0.97      0.94      0.95       463\n",
            "\n",
            "    accuracy                           0.96       988\n",
            "   macro avg       0.96      0.96      0.96       988\n",
            "weighted avg       0.96      0.96      0.96       988\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch0w2fRmHYYo"
      },
      "source": [
        "ngram_range=(3, 3) — триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuAFAOBiUzs2"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(3, 3))\n",
        "bow = vec.fit_transform(X_train) \n",
        "bow_test = vec.transform(X_test)\n",
        "\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bub2cAPU29w",
        "outputId": "886b732e-78a8-4933-f938-c59ed70bab2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.31      0.47       525\n",
            "           1       0.56      1.00      0.72       463\n",
            "\n",
            "    accuracy                           0.63       988\n",
            "   macro avg       0.77      0.65      0.59       988\n",
            "weighted avg       0.79      0.63      0.59       988\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred_thrgramm = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred_thrgramm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pk_VGalHgX4"
      },
      "source": [
        "Для 3 грамм намного меньше f1-score, чем для 1 грамм."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4oOPsEtU9Aa"
      },
      "outputs": [],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "vec_train = vec.fit_transform(X_train)\n",
        "vec_test = vec.transform(X_test)\n",
        "\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "vec_train = scaler.fit_transform(vec_train)\n",
        "vec_test = scaler.transform(vec_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyCYu5v9VHuJ",
        "outputId": "2a77bb86-ab33-47a7-fe5a-cf7c2ecbca6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95       525\n",
            "           1       0.97      0.93      0.95       463\n",
            "\n",
            "    accuracy                           0.95       988\n",
            "   macro avg       0.95      0.95      0.95       988\n",
            "weighted avg       0.95      0.95      0.95       988\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter=300, random_state=42)\n",
        "clf.fit(vec_train, y_train)\n",
        "pred_tfidf = clf.predict(vec_test)\n",
        "print(classification_report(y_test, pred_tfidf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWHpfd1pHyth"
      },
      "source": [
        "Раньше мы принимали пунктуацию за шум. Давайте посмотрим, что будет если не убирать ее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSEszfyVVMxs"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(X_train) \n",
        "bow_test = vec.transform(X_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5LHKMVzVpyy",
        "outputId": "5a89a4b9-9af0-4466-efde-b437289e97ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.96      0.95       525\n",
            "           1       0.96      0.94      0.95       463\n",
            "\n",
            "    accuracy                           0.95       988\n",
            "   macro avg       0.95      0.95      0.95       988\n",
            "weighted avg       0.95      0.95      0.95       988\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M42S84bLIHv8"
      },
      "source": [
        "С пунктацией скор немного уменьшился. В некоторых задачах пунктуациях может многое сказать о тональности высказывания. Например, значок `)` может указывать о положительном отзыве."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_jZkHJIhqI"
      },
      "source": [
        "Попробуем в качестве признаков использовать униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTqb99ztV7L7"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), analyzer='char')\n",
        "bow = vec.fit_transform(X_train) \n",
        "bow_test = vec.transform(X_test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwyDIg1wV-Rz",
        "outputId": "d3eeb69d-b726-4a5d-92f4-1d33792724d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.69      0.77       525\n",
            "           1       0.72      0.90      0.80       463\n",
            "\n",
            "    accuracy                           0.79       988\n",
            "   macro avg       0.80      0.79      0.79       988\n",
            "weighted avg       0.81      0.79      0.79       988\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(bow_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MscbDOJBWBPt"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9sqzEGrIujm"
      },
      "source": [
        "Попробуем осуществить следующую логику. Возьмем предобученную нейронку, подадим ей наши отзывы, она вернет нам числовое описание наших отзывов, мы попадим их нашему класссификатору."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "If9nhVn6WD2W",
        "outputId": "f6da0f3f-0b69-4b8e-9e2e-53e5e96fc0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-01 05:40:23--  http://files.deeppavlov.ai/deeppavlov_data/bert/sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz\n",
            "Resolving files.deeppavlov.ai (files.deeppavlov.ai)... 178.63.27.41\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.deeppavlov.ai/deeppavlov_data/bert/sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz [following]\n",
            "--2022-11-01 05:40:23--  https://files.deeppavlov.ai/deeppavlov_data/bert/sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz\n",
            "Connecting to files.deeppavlov.ai (files.deeppavlov.ai)|178.63.27.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 661614603 (631M) [application/octet-stream]\n",
            "Saving to: ‘sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz’\n",
            "\n",
            "sentence_ru_cased_L 100%[===================>] 630.96M  19.7MB/s    in 34s     \n",
            "\n",
            "2022-11-01 05:40:57 (18.8 MB/s) - ‘sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz’ saved [661614603/661614603]\n",
            "\n",
            "sentence_ru_cased_L-12_H-768_A-12_pt/\n",
            "sentence_ru_cased_L-12_H-768_A-12_pt/pytorch_model.bin\n",
            "sentence_ru_cased_L-12_H-768_A-12_pt/bert_config.json\n",
            "sentence_ru_cased_L-12_H-768_A-12_pt/vocab.txt\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeppavlov\n",
            "  Downloading deeppavlov-0.17.6-py3-none-any.whl (878 kB)\n",
            "\u001b[K     |████████████████████████████████| 878 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting numpy==1.18.0\n",
            "  Downloading numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting uvicorn==0.11.7\n",
            "  Downloading uvicorn-0.11.7-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 54.4 MB/s \n",
            "\u001b[?25hCollecting pandas==0.25.3\n",
            "  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 45.1 MB/s \n",
            "\u001b[?25hCollecting pyopenssl==22.0.0\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 151 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Collecting prometheus-client==0.7.1\n",
            "  Downloading prometheus_client-0.7.1.tar.gz (38 kB)\n",
            "Collecting pytz==2019.1\n",
            "  Downloading pytz-2019.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.2\n",
            "  Downloading scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7 MB 50.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "  Downloading sacremoses-0.0.35.tar.gz (859 kB)\n",
            "\u001b[K     |████████████████████████████████| 859 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.17.3)\n",
            "Collecting overrides==2.7.0\n",
            "  Downloading overrides-2.7.0.tar.gz (4.5 kB)\n",
            "Collecting requests==2.22.0\n",
            "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting aio-pika==6.4.1\n",
            "  Downloading aio_pika-6.4.1-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 23 kB/s \n",
            "\u001b[?25hCollecting pymorphy2==0.8\n",
            "  Downloading pymorphy2-0.8-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting fastapi==0.47.1\n",
            "  Downloading fastapi-0.47.1-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml==0.15.100\n",
            "  Downloading ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654 kB)\n",
            "\u001b[K     |████████████████████████████████| 654 kB 71.1 MB/s \n",
            "\u001b[?25hCollecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 53.9 MB/s \n",
            "\u001b[?25hCollecting uvloop==0.14.0\n",
            "  Downloading uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 51.6 MB/s \n",
            "\u001b[?25hCollecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.62.0\n",
            "  Downloading tqdm-4.62.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting rusenttokenize==0.0.5\n",
            "  Downloading rusenttokenize-0.0.5-py3-none-any.whl (10 kB)\n",
            "Collecting pytelegrambotapi==3.6.7\n",
            "  Downloading pyTelegramBotAPI-3.6.7.tar.gz (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting Cython==0.29.14\n",
            "  Downloading Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 47.8 MB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting pydantic==1.3\n",
            "  Downloading pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 58.0 MB/s \n",
            "\u001b[?25hCollecting aiormq<4,>=3.2.0\n",
            "  Downloading aiormq-3.3.1-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (1.8.1)\n",
            "Collecting starlette<=0.12.9,>=0.12.9\n",
            "  Downloading starlette-0.12.9.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 51.0 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting cryptography>=35.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 34.3 MB/s \n",
            "\u001b[?25hCollecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov) (1.2.0)\n",
            "Collecting websockets==8.*\n",
            "  Downloading websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting httptools==0.1.*\n",
            "  Downloading httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219 kB)\n",
            "\u001b[K     |████████████████████████████████| 219 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting pamqp==2.3.0\n",
            "  Downloading pamqp-2.3.0-py2.py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=35.0->pyopenssl==22.0.0->deeppavlov) (2.21)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (4.1.1)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (6.0.2)\n",
            "Building wheels for collected packages: nltk, overrides, prometheus-client, pytelegrambotapi, sacremoses, docopt, starlette\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449924 sha256=dcc350402b0374b6c46c3183c3c0bd52c832ee9a34b4d956e4c89997bd6894e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.7.0-py3-none-any.whl size=5603 sha256=29334607c4eacf5d121182521c67147573076481bc6b81c4d73ff93720a1a527\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/87/45/bfdacf6c3b8233b6e8d519edcbd1cf297ad5ff5f0bf84bb9c1\n",
            "  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-py3-none-any.whl size=41405 sha256=d4762852b52460e8fbe23ff3e9109f2ffb093d57dd58b14365e06532cfd3041f\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/0c/26/59ba285bf65dc79d195e9b25e2ddde4c61070422729b0cd914\n",
            "  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-py3-none-any.whl size=47176 sha256=6529e8d4604995530744dd6c06dafcf813602fc513a91e1be97241a007df42ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/7c/54/8eddf2369ef1b9190e2ee6dc2b40df54b6c65529a38790fdd4\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-py3-none-any.whl size=883989 sha256=6448dcbcea1e664953604b90e303bb18a8b2d41707f16ab763292993bdfe2586\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ff/0e/e00ff1e22100702ac8b24e709551ae0fb29db9ffc843510a64\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=13c03a3bea97c5b6027d2defe9177ca5cb74f44b55b3496c3e9799db78881456\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for starlette: filename=starlette-0.12.9-py3-none-any.whl size=57252 sha256=f82d55a11300a0747ff1fa26eae1928257341bbfe9a67e738460027ff9266c71\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/78/be/f57ed5aed7cd222abdb24e3186b5c9f1074184fcc0a295102b\n",
            "Successfully built nltk overrides prometheus-client pytelegrambotapi sacremoses docopt starlette\n",
            "Installing collected packages: idna, pamqp, numpy, websockets, uvloop, tqdm, starlette, scipy, requests, pytz, pymorphy2-dicts, pydantic, httptools, h11, docopt, dawg-python, cryptography, aiormq, uvicorn, scikit-learn, sacremoses, rusenttokenize, ruamel.yaml, pytelegrambotapi, pyopenssl, pymorphy2-dicts-ru, pymorphy2, prometheus-client, pandas, overrides, nltk, h5py, filelock, fastapi, Cython, aio-pika, deeppavlov\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.7.3\n",
            "    Uninstalling scipy-1.7.3:\n",
            "      Successfully uninstalled scipy-1.7.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2022.5\n",
            "    Uninstalling pytz-2022.5:\n",
            "      Successfully uninstalled pytz-2022.5\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.8.0\n",
            "    Uninstalling filelock-3.8.0:\n",
            "      Successfully uninstalled filelock-3.8.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.32\n",
            "    Uninstalling Cython-0.29.32:\n",
            "      Successfully uninstalled Cython-0.29.32\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.21.2 which is incompatible.\n",
            "xarray 0.20.2 requires pandas>=1.1, but you have pandas 0.25.3 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.0 which is incompatible.\n",
            "thinc 8.1.5 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
            "tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.18.0 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.0 which is incompatible.\n",
            "spacy 3.4.2 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
            "prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.0 which is incompatible.\n",
            "plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "plotnine 0.8.0 requires scipy>=1.5.0, but you have scipy 1.4.1 which is incompatible.\n",
            "mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.18.0 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.18.0 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.18.0 which is incompatible.\n",
            "jax 0.3.23 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests>=2.23.0, but you have requests 2.22.0 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.18.0 which is incompatible.\n",
            "confection 0.0.3 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 1.3 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.18.0 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.18.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-38.0.1 dawg-python-0.7.2 deeppavlov-0.17.6 docopt-0.6.2 fastapi-0.47.1 filelock-3.0.12 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 idna-2.8 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-22.0.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 scipy-1.4.1 starlette-0.12.9 tqdm-4.62.0 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk",
                  "numpy",
                  "pandas",
                  "pytz",
                  "scipy",
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 72.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
          ]
        }
      ],
      "source": [
        "! wget http://files.deeppavlov.ai/deeppavlov_data/bert/sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz\n",
        "!tar --gunzip --extract --verbose --file=\"sentence_ru_cased_L-12_H-768_A-12_pt.tar.gz\"\n",
        "!pip install deeppavlov\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf1GDVs4WWhz",
        "outputId": "a54fde86-6a96-473b-87a5-ee6bf2c113c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "Some weights of the model checkpoint at /content/sentence_ru_cased_L-12_H-768_A-12_pt were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from deeppavlov.core.common.file import read_json\n",
        "from deeppavlov import build_model, configs\n",
        "import torch\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "noise = stopwords.words('russian') + list(punctuation)\n",
        "# bert\n",
        "bert_config = read_json(configs.embedder.bert_embedder)\n",
        "bert_config['metadata']['variables']['BERT_PATH'] = 'sentence_ru_cased_L-12_H-768_A-12_pt'\n",
        "\n",
        "m = build_model(bert_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "os_wHty6ZyGg"
      },
      "outputs": [],
      "source": [
        "list_name = [0] * len(X_train)\n",
        "for i in range(len(list_name)):\n",
        "  list_name[i] = X_train[i].translate(noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH4sPlAlaemA",
        "outputId": "a4632065-6662-41e7-9b33-222018b8d548"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 434/2963 [00:14<00:37, 67.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ДобрыйотденьменяотбытьнегоуженибылужебытьужотвотчатеотмобильногоотприложенияотБанкаотТинькоффотнаписалаотпоотповодуотсменыотадресаотпроживаниялиотаотточнееотпоинтересоваласьотвозможноотлиотизотвкладкиот?адреса»отубратьотнеактуальныйотадресоткогдаприотэтомотнаправилаотещеотиотскриншотлиотобведяоткраснымоттолиотчтоотследуетотскорректироватьдажеейотСотрудникотБанкаотВалентинаотспросилаотсменилаотлиотяотадресотпроживанияотиот?ушлаотпроверять»ужеотСтраннолиотчтоотсотрудникотпослеотзавешиванияотклиентаотнеотпишетотстандартноеот?спасибоотзаотожидание»отиотвсёотвотэтомотдухеототужеотНоотэтоотладнолиотдалееотпослеотмоегоотответаот?да»отнаотвопросотВалентиныот?удобноотлиотпринятьотзвонокотототбанкаотдляотизмененияотданных»лиотвотдиалоготвступаетотбылеслийотсотрудникот–отВикториялиоткотораяотдажеотНЕотПОЗДОРОВАВШИСЬотототпишетлиотчтоотейотнеобходимоотознакомитьсяотсотпредыдущейотперепискойужеотАотгдеотжеотобещанныйотзвонокотототБанкаейотНоотнаотэтомот?веселье»отнеотзаканчиваетсяужеотСпустяотпаруотминутотВикторияотвозвращаетсяотиотприсылаетоткакойеслитоотнеотимеющийоткотмоемуотвопросуотшаблонныйотответотооттомлиотчтоот?необходимоотинформироватьотБанкотоботизмененияхлиотпоотУКБОотвоттечениеотопятьоткалендарныхотднейотписьменноотинформироватьотБанк»ужеотАотзатемотиотвовсеотприходитоткотнеожиданномуотзаключениюлиотаотименноот?какотяотпонялаотизотперепискиотвышелиотуотВасотизменилсяотадресотрегистрацииотиотпроживаниялиотверноей»откогдаНеотдождавшисьотответаотототменялиотприслалаотВикторияотсписоклиотчтоотнужноотсделатьотдляоттоголиотчтобыотизменитьотадресотрегистрациидажеотХм…отВидимоотнеотмоюотперепискуотчиталотсотрудникужеотототПослеотмоегоотответалиотчтоотизменилсяотадресоттолькоотпроживаниялиотмнеотВикторияотпредложилаотбылотвариантаведьотизменитьотпрямоотсейчасотвотчатеотилиотпозвонивотпоотбесплатномуотномеруотвотБанкужеотЯотпредпочлаотбытьеслийотизотдвухотпредложенныхотвариантовлиототметивлиотчтоотнынеотадресотпроживанияотсовпадаетотсотадресомотрегистрацииужеотИотзнаетелиотчтоотмнеотответилаотВикторияейотЧтоот?решениеотданногоотвопросаотосуществляетсяоттолькоотпоотзвонкуотвотБанк»ужеотСюрприиииздажедажедажеотАотдалееотуточнилалиотсогласнаотлиотяотпринятьотзвонокотСЕЙЧАСужеотДа…согласнаужеотПарамеслипарамеслипамменяотототВикторияотитутотменяот?обрадовала»ведьотожидайтелиотпожалуйсталиотяотпозвонюотВамотвотближайшееотвремяужеотВотмоемотпониманииот?сейчас»отеслиотэтоотсейчасоткогдакэпдажеотлибоотвоттечениеотбытьеслибылотминутужеотИотчтоотэтоотзаотзагадочноеот?ближайшееотвремя»лиотмнеотнеотуточнилиужеотПолчасалиотчасейотВотитогелиоткогдаотяотужеотбылаотзаняталиотэтоотсамоеот?сейчас»отиот?ближайшееотвремя»отнаступилоужеотСпустяотбытьвамеслибылниотминутотпослеотперепискиужеотМда…отНеотдозвонившисьотдоотклиенталиотменяотнеотпроинформировалиотвототкрытомотранееотчатеотиотпосредствомотсмсоттожеотнеотуведомилиужеотЗатоотспустяотбытьниотминутотпослеотнепринятогоотзвонкаотмнеотнаправилиотписьмоотнаотпочтуужеотЛеслилогика…отототКлиентотобратилсяотвотчателиотзвонкаотожидалот?сейчас»лиотаотемуотприслалиотписьмоотнаотэлектронныйотадресоткогдаписьмолиототмечулиотяотпрочлаотзначительноотпозжедажеужеотСпустяотчасотсотлишнимлиотнеотдождавшисьотповторногоотзвонкалиотяотпозвонилаотвотБанкотсамаужеотСотрудникотАлександраотсотгоремотпополамотмнеотизменилаотадресотпроживанияужеотНаотвсёотэтоотушлоотужотминутоткогдаменядажелиотаотведьотэтоотпростейшийотвопрослиотребятаменяотПриотэтомотмнеотпришлоотажотцелыхотнегоотсмсоткогдасотразницейотвотбылотминутыоткаждаядажеотооттомлиотчтоот?Вашиотданныеоткогдаадресотпроживаниядажеотизменены»ужеотВидимоотпрекраснаяотАлександраотнегоотразаотэтиотсамыеотданныеотменяла…отНулиотдумаюлиотславаотБогуот–отвсёеслитакиотпоменялиужеотИотнеотпроверилаотданныеотвотМПлиотилиоткакотоказалосьлиотзряужеотНаэтомотнеотвсёведьотспустяотбытьужевасотчаса*послеотмоегоотобращениялиотмнеотвновьотпозвонилиотизотБанкаужеотИотзнаетеотзачемейотЧтобыотпоменятьотадресотпроживания…отКакотзвалиотсотрудникаотялиотувылиотнеотпомнюлиоттужекужеотрешениемотмоегоот?сложнейшего»отвопросаотктооттолькоотужеотнеотзанималсяужеотВечеромлиотзайдяотвотМПлиотяотбылаотошарашенаведьотототадресотпроживанияотизменилилитолькоотиндексотоставилиотпрежнимлиотнеотимеющимотсовершенноототношенияоткоттекущемуотадресу*отвсехчтобыведьототЧтоотможетотбытьотпрощелиотчемотизменитьотадресотпроживанияотнаотсовпадающийотсотадресомотрегистрацииейотВидимоотдляоттоголиотчтобыоттеперьотэтототнесчастныйотиндексзаменитьлиотсоотмнойотещеотвасотсотрудниковотБанкаотсвяжутсяужеотототЕслиотвыотздесьотвоттрехотсоснахотзаблудилисьлиоттоотможноотлиотвамотдоверятьотрешениеотсложныхлиотиотужоттемотболееотфинансовыхотвопросовей']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 791/2963 [00:19<00:33, 65.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ЕслиотвыотдорожитеотсвоимиотнервамиотеслиотниотприоткакихотобстоятельствахотнеотимейтеотделотсоткредитнымиотпродуктамиотэтогоотбанкаменяВотТинькоффеслибанкеотуотменяотбылаотоформленаоткредитнаяоткартаотиотпотребительскийоткредитужеотКредитотяотоформилаотвотавгустелиотпоотграфикуотплатежейотя*должнаотбылаотвноситьотпоотнемуотдоужбылниотрубужеотопятьеслигоотчислаоткаждогоотмесяцалиотначинаяотс*сентябряужеКредитнойоткартойотгодотнеотпользоваласьотвообщелиотвотиюлеотэтогоотгодаотснялаотсотнееотбылниотнининиоттысужеотбытьотавгустаотполностьюотпогасилаотвсюотсумму*задолженностилиоткоторуюотнаоттототмоментотвиделаотвотсвоемотинтернетеслибанке*когдадолгвдругпроцентыдажеотиотбылаотполностьюотувереналиотчтоотпооткредиткеотяотбольшеотничегоотне*должнауже*былнибудьотсентябряотмнеотпоступаетотпервоеотзаотвсеотвремяотсмсеслисообщениеотбанкалиотчтоотуотменяотпросроченотплатежотпооткредитнойоткартеотнаотсуммуотбытьдонегоотрубужеотиотбудет*начисленотштрафужеотЯотниотразуотвотжизниотнеотдопускалаотпросрочекотпооткредитамлиотили*конечнолиотбылаотвотшокеужеотЗвонюотвотбанкотиотвыясняюлиотчтоотужеотпослеотвнесенияотмной*платежаотпроцентыотпооткредиткеотбылиотдоначисленыуже*Никакихотоповещенийотяотоб*этомотнеотполучалаотеслиотравнооткакотиотникакихотпредварительныхотнапоминанийото*необходимостиотпогашениялиоткоторыеототправляютотсвоимотклиентамотвсеотнормальные*банкиужеПоотутверждениюотсотрудниковотбанкаотмнеотприходилиотпушеслиуведомленияужеотЯотпроверилаотсписокотпушеслиуведомленийотвотсвоемотинтернетеслибанкеотеслиотни*единогоотуведомленияоттамотнеотзафиксированоужеЯотвоттототжеотденьотпогасилаотэтуотсуммуотиотзакрылаоткредитнуюоткартуужеотОднакоотнаотэтомотделоотнеотзакончилосьужебылвамотсентябряотмнеотзвонитотототсотрудникототделаотвзысканийлиоткоторыйотвотультимативнойотформеотсообщаетотмнелиотчтоотуотменяотпросроченотплатежотпо*потребительскомуоткредитулиотначисленотштрафлиотиотяотобязанаотсегодняотжеотвнестиотвотбанк*васбылбылдоотрубужеотЯлиотопятьотжелиотвотшокелиоттакоткакотпредыдущийотплатежотпоотэтомуоткредитуотв*размереотдоужбылниотрубужеотвнеслаотвасотсентябрялиотиотабсолютноотувереналиотчтоотникаких*просроченныхотзадолженностейотуотменяотнетужеотНичегоотвнятногоотпоотповодуоттоголиоткакимотобразомуотменяотопятьотобразоваласьоткакаяеслитоотпросроченнаяотзадолженностьлиотпозвонившийотмне*сотрудникотнеотсообщилужеВернувшисьотдомойлиотяотподнялаотдокументыотиотубедиласьлиотчто*отдатаотследующегоотплатежаотуотменяотдействительноотопять*октябряуже*отЭтаотжеотинформацияотбылаотвотмоемотинтернетеслибанкеуже*ЯотпозвонилаотвотконтактеслицентротбанкаужеотСотрудникиотдолгоотразбиралисьлиотвотконцеотконцовотмнеотсказалили*чтоотменяотещенеправильноотпроконсультировалиещеоткогдаэтооттакотмягкоотониотназвали*наездотихотсотрудникаототделаотвзысканиядажеужеотТакжеотониотобъяснилилиотчтоотпоотдоговоруото*комплексномотобслуживанииотвотслучаелиотеслиотдопускаетсяотпросрочкаотпоотодномуотизкредитныхотпродуктовлиоточереднойотплатежотпоотостальнымотпродуктамотмне*ещерекомендуетсяещеотвноситьотзаранееужеотТакотчтоотяотдолжнаотвнестиотсегодня*суммуоточередногоотплатежаотпоотмоемуоткредитуотвотразмереотдоужбылниотрубужелиотаотинформацияото*начисленномотмнеотштрафеотбылаотошибочнойужеОклиотденьгиотяотвнеслалиотхотяотэтоотиотбылооткрайнеотнеудобноотеслиотведьотяотрассчитывалаотвнестиотэтуотсуммуотнаотнеделюотпозднееужеотКогдаотяотвнеслаотнаотсчет*обслуживанияоткредитнойоткартыотуказаннуюотсуммулиотмнеотприходитотсмслиотчтоотяотдолжна*банкуотещеотдонидоотрубужеотЯотопятьотзвонюотвотбанклиотиоттребуюотвыслатьотмнеотофициальныйдокументотсотпечатьюлиотчтоотнаотсегодняшнийотденьотуотменяотникакихотпросроченных*задолженностейотвотэтомотбанкеотнетуже*ВотитогеотполовинаотвыходногоотдняотпотраченаотнаотразборкиотсотсотрудникамиотбанкалиотнастроениеотбезнадежноотиспорченоотиотнервыотнаотпределеужеотПри*этомотличноотуотменяотвозниклоотвпечатлениелиотчтоотбанкотнамеренноотспровоцировалвозникновениеотэтойотпросрочкиотпооткредиткеотдляоттоголиотчтобыотплатежиотпо*потребительскомуоткредитуотсотменяотраньшеотсрокаоттребоватьуже*ЗаотбылниотлетотактивногоотпользованияоткредитнымиотпродуктамиотразныхотбанковотвоттакуюототвратительнуюотситуациюпопадаюотвпервыеужеотРазумеетсялиотбольшеотникакихотделотяотсотэтимотбанкомотиметьотнеотбудуотесли*иотникомуотнеотсоветуюменя']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 1040/2963 [00:23<00:40, 47.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ОтветилотнаотопросотбанкаотоботиспользованииотвкладовотиотполучилотвотконцеотпредложениеотнаписатьототзывотнаотэтомздесьникогдакудасейчасужедвахотьужеотАотпочемуотбыоти*нетей*Пользуюсьотбанкомот?Тинькофф»отнесколькоотлетужеотНачалосьотзнакомствоотсотнимотсотжеланияототкрытьотвклад*когдапроцентотбылотпривлекательнымдажеужеотДляототкрытияотвкладаот?пришлось»отзавестиотдебетовуюоткартуоттутзачемздесьодинкудалиоткоторуюотпривёзотмнеотдомойотвотудобноеотмнеотвремяоткурьеружеототПотомотрешилотпопробоватьотиспользоватьотданнуюоткартуотдляотоплатыотвотмагазинахлиотчемуотспособствовалаотпредложеннаяоткатегорияотсотповышенным*кэшбэкомот?Супермаркеты»отужеотОткрыл*дополнительнуюоткартуотженеотужеотКатегория*?Супермаркеты»отпотомотпропалалиотноотпривычкаотосталасьотужеотСпустяотнекотороеотвремя*открылоткредитнуюоткартуот?онизачемзачемонисейчасдвазачемсейчасникогдамойоб»откогдаопять*такиотизотмеркантильныхотсоображенийлиотдлянакопленияотмильотототдажеужеототВотрезультатеотбанк*вотнастоящийотмоментотявляетсяотосновнымотрасчётнымотвотсемьеужеотВпечатленияотположительныеужеотУстраиваетотвотцеломотвсёужеотАотпослеотустановкиотбанкоматаот?Тинькофф»*рядомотсотдомомотвообщеотжизньотналадиласьотужеототВотнастоящийотмоментотиспользуюотбанкотдляотцелогоотродаотоперацийот—отдебетовыеотиоткредитныеоткогдаосновнаяотиотдополнительныедажеоткартылиотвкладылиотоплатаотЖКУотпоотсчетамлиотпоявляющимсяотвотинтернетеслибанкелиотбронированиеототелейоткогдасотпереходомотнаотэтомможноможнокудасейчасникогдачтобыужеодинможновсехдажеужеотИзот?плюсов»ведьотудобнаяотподдержкаотпооттелефонуотиотвотТелеграммеоткогдамнеотличноотудобнееотписатьиличитатьли*чемотразговариватьдажелиотвозможностьототкрытия*несколькихотнакопительныхотсчетовоткогдадляотразныхотцелейдажелиотработающаяотпрограммаотлояльностиоткогдаужеоткомпенсировалоткупленныеотавиабилетыотнакопленнымиотмилямидажелиотдаотиотпростоотбеспроблемная*работаоткартужеотНаверноелиотпроотхорошееотписатьотсложноот*еслиотвсёотпростоотработаетужеототИзот?шероховатостей»ведьотнемногоотраздражает*необходимостьотснятияотминимальнойотсуммы*вотнегонининиотрублейотвотстороннихотбанкоматахотвоотизбежаниеоткомиссииотеслиоткогдаотнужнаотнебольшаяотсуммаотналичнымилиотприходитсяотснимать*негонининилиотаотостатокотпотомотопятьотвозвращатьотнаотТинькоффужеотЕщёотхотелосьотбыототустановитьот?поотумолчанию»отлимитотнаотмесяцлиотаотто*получаетсяотситуациялиоткогдаотпослеоткрупныхотпокупокоткогдаспециальноотподнимаетсяотлимитдаже*сотбытьотчислаотследующегоотмесяцаототдоступнаотстановитсяотнеприличноотбольшаяотсумма*ужеототЕстьотмелкиеотнедоработкиотвотинтернетеслибанкеведь*неотсоотвсехотквитанцийотсчитывается*наконецдваесликодотдляотоплатылиотнолиотхочетсяотверитьлиотэтоотисправятотвотближайшемотбудущемужеотШирокоотобсуждаемоеототсутствиеотофисовотбанка*мнеотличноотникакотнеотмешаетужеотНаоборотлиотудобнееотобсудитьилирешитьотвопросыотпосредствомотразговораотилиотпереписки*когдасмужеотвышедажеужеотНиотразуотнеотстолкнулсяотсотситуациейлиоткогдаотпожалелотбыотобототсутствии*возможностиотзайтиотвотофисотбанкаотОпираясьотнаотопытотобщенияотсотбанкомлиотсталотклиентомотиотстраховойоткомпанииот?Тинькофф»отужеотСейчас*вототсотрудничествоотсотнимиотпроходитотопытот?боевогооткрещения»ужеотБудуотремонтировать*автомобильотпоотОСАГОотпоотнаправлениюотиз*?ТинькоффотСтрахования»ужеотДумаюлиотпо*результатамотнапишуототдельныйототзывотуже*вотдругойотразделотфорумаужеототНеотбудуотспоритьотсотвозможнымиотутверждениямилиотчтоотиотвкладыотнаотрынкеотестьотболееотинтересные*иоткэшбэкотуот?Тинькофф»отнеотсамыйотбольшойуже*Нолиотнаотмойотвзглядлиотгонкаотзаотнаиболееотвыгоднымиотпредложениямиотявляетсяотсвоего*родаотхоббилиоткотороеоттребуетотпостоянного*отслеживанияотрынкаотиотперестройки*использующихсяотвотсемьеотфинансовыхотмеханизмовужеотЯотличноотнеотготовилинеотхочу*этимотзаниматьсяужеотПериодическиотподнимаемыеотвответкеотобсужденияотбанкаоттемыотнарушенияотимотзаконодательства*меняотнеотзатронулиотужеототРезюмируяотсвойототзывот—отменяотбанкотнаоттекущийотмоментотустраиваетоткогдазагадыватьотнаотбудущееотнеотбудуотдажелиоткакихеслилибоотоснованийотдляотегоотсменыотнеотвижуужеотОпятьеслитакиотэтоотличноеотмнениелиотопирающеесяотнаотличныеотпотребностиилипредпочтенияуже']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 2096/2963 [00:45<00:12, 68.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ВотоктябреотбылнибытьужотгодаотпутешествовалотсотсемьейотпоотИзраилюужеотВзялотвотпрокатотвоткомпанииоттутподраздлянадосебеотавтомобильужеотИспользовалотвоткачестве*отрасчета*откредитнуюоткартуотТинькофф*отонибылабылаотонитебяразбылатебячтобнадотожеужеотСчитаю*отэту*откарту*оточеньотудачнойотдля*отпутешествийлиоттакоткакотпоотней*отповышенный*откешбекотнаотбилетыотиотбронироваие*ототелейотнаоттутбезбезчемтебячтобдляуже*ПутешествиеотудалосьужеотНоотвидимоотгдееслитоотвотИерусалимеотвсеоттаки*отнарушил*отправилаотдорожногоотдвижениялиотприпарковалотмашинуотв*отнеположенном*отместеуже**Вотитогеоттутподраздлянадосебеотоказаласьоточень*отхитрожужеужеужейоткомпаниейужеотЯотзабронировалотдля*отсемьи*мытотхотьникогдапочтиздесьсейчасотсебехотьсобоникогдаужеотвотаэропорту*отмнеотменеджер*отсказаллиотчто*оттакойотмашины*отнетужеотИотпредложила*самсейчасдругойобхотьэтомсейчасобнеесейчасотонитожетогдаужеотЧтоотменя*отнеотустроилоужеотВотитоге*отпосле*отдлительных*отпереговоровотнашли*мытотхотьникогдапочтиздесьсейчасотсебехотьсобоникогдауже*После*отокончанияотсрока*отарендылиотсчет*отблагополучноотувеличился*отнаотбытьвасниоужеотТут*отже*отприлетел*отштрафотбытьвасоужеотДабы*отнеотомрачать*ототпускотя*отзаплатил*отэти*отденьгиоти*отзабыл*оттутподраздлянадосебелиоткакотстрашный*отсонуже*Нооттутподраздлянадосебе*отне*отзабыл*отпроотменяужеотВидимотим*отпонравилосьотснимать*отденьгиотс*откартыужеотЧерезотполгодаотониотсписали*отещеотбытьвасоотза*оттот*отже*отсамый*отштрафуже*Яотнаписалотв*оттехподдержкуотТинькоффотбанклиотсотпросьбой*ототменить*отданный*отплатежужеотПрошлоотнесколько*отдней*отсотрудники*отразобралисьотв*отситуацииоти*отденьги*отвернулись*отнаотсчет*откартыужеотВыражаю*отблагодарность*отсотрудникамотТинькоффотБанклиотзаототзывчивостьотиотпроффесианолизмуже']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2963/2963 [01:00<00:00, 48.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "res = []\n",
        "error = []\n",
        "with torch.no_grad():\n",
        "  for j in tqdm(range((len(list_name)))):\n",
        "    try:\n",
        "      l_batch = list_name[j: (j+1)]\n",
        "      tokens, token_embs, subtokens, subtoken_embs, sent_max_embs, sent_mean_embs, bert_pooler_outputs = m(l_batch)\n",
        "      res.append(sent_mean_embs)\n",
        "      torch.cuda.empty_cache()\n",
        "    except:\n",
        "      print(l_batch)\n",
        "      error.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-d8CEOvToYq",
        "outputId": "60f3c987-c453-474e-abca-b30932846546"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2957/2957 [00:03<00:00, 921.69it/s]\n"
          ]
        }
      ],
      "source": [
        "res_last = res[-1][:, :1000]\n",
        "for j in tqdm(range(len(res) - 1)):\n",
        "  res_last = np.concatenate((res[j][:, :1000], res_last))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mbnhh_Ot5RT",
        "outputId": "2f569e96-29e3-4899-b7d9-ef40efa9824f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.28      0.41      1448\n",
            "           1       0.57      0.92      0.70      1510\n",
            "\n",
            "    accuracy                           0.60      2958\n",
            "   macro avg       0.67      0.60      0.55      2958\n",
            "weighted avg       0.66      0.60      0.56      2958\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter=200, random_state=42)\n",
        "clf.fit(res_last, y_train.drop(error))\n",
        "pred = clf.predict(res_last)\n",
        "print(classification_report(y_train.drop(error), pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxGKuZTAuwg1"
      },
      "outputs": [],
      "source": [
        "# Как мы видим даже на тренировочной выборке результат намного ниже, чем в случай CountVectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io1moCXUwoHU"
      },
      "source": [
        "Обратимся к Достоевскому. Это библиотека для анализа тональности текстов на русском языке. Модель обучалась на наборе данных RuSentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDNyXgGgwmOU",
        "outputId": "9c934b80-9703-41be-8959-d3aecc353004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dostoevsky\n",
            "  Downloading dostoevsky-0.6.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting fasttext==0.9.2\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting razdel==0.5.0\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2->dostoevsky) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2->dostoevsky) (1.18.0)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3161585 sha256=5bf3d612b9456be62d2fe2114e119a13f484e139570a6cfb58ba524ba94ed6c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, razdel, fasttext, dostoevsky\n",
            "Successfully installed dostoevsky-0.6.0 fasttext-0.9.2 pybind11-2.10.1 razdel-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dostoevsky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X1bOm9TXwyQj"
      },
      "outputs": [],
      "source": [
        "!python -m dostoevsky download fasttext-social-network-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgKD0Bnfw_Al",
        "outputId": "48a249dc-1800-44f4-c9d0-d90ed290f543"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "from dostoevsky.tokenization import RegexTokenizer\n",
        "from dostoevsky.models import FastTextSocialNetworkModel\n",
        "\n",
        "tokenizer = RegexTokenizer()\n",
        "model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
        "\n",
        "results = model.predict(X_test, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "CN2Q4rMBxFEy"
      },
      "outputs": [],
      "source": [
        "pred = []\n",
        "for j in range(len(results)):\n",
        "  if 'negative' in list(results[j].keys())[0]:\n",
        "    pred.append(0)\n",
        "  elif 'positive' in list(results[j].keys())[0]:\n",
        "    pred.append(1)\n",
        "  elif 'negative' in list(results[j].keys())[1]:\n",
        "    pred.append(0)\n",
        "  elif 'positive' in list(results[j].keys())[1]:\n",
        "    pred.append(1)\n",
        "  # выбор\n",
        "  elif 'neutral' in list(results[j].keys())[0] and 'skip' in list(results[j].keys())[1]:\n",
        "    pred.append(1)\n",
        "  elif 'skip' in list(results[j].keys())[0] and 'neutral' in list(results[j].keys())[1]:\n",
        "    pred.append(1)\n",
        "  elif 'speech' in list(results[j].keys())[0] and 'neutral' in list(results[j].keys())[1]:\n",
        "    pred.append(1)\n",
        "  elif 'neutral' in list(results[j].keys())[0] and 'speech' in list(results[j].keys())[1]:\n",
        "    pred.append(1)\n",
        "  elif 'speech' in list(results[j].keys())[0] and 'skip' in list(results[j].keys())[1]:\n",
        "    pred.append(1)\n",
        "  elif 'skip' in list(results[j].keys())[0] and 'speech' in list(results[j].keys())[1]:\n",
        "    pred.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWgUrJyW14pX",
        "outputId": "93cfeafc-0bd1-44ec-eb20-10bd1fe6c613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.96      0.75       525\n",
            "           1       0.88      0.30      0.45       463\n",
            "\n",
            "    accuracy                           0.65       988\n",
            "   macro avg       0.75      0.63      0.60       988\n",
            "weighted avg       0.74      0.65      0.61       988\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlE4-OOQ2gtb"
      },
      "outputs": [],
      "source": [
        "# Можно выбирать конфигурации того, как мы кодируем нейтральность, скип и разговорный."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
